{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import pickle\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from matplotlib.pyplot import specgram\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os # interface with underlying OS that python is running on\n",
    "import sys\n",
    "import warnings\n",
    "# ignore warnings \n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import keras\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.models import Sequential, Model, model_from_json\n",
    "from keras.layers import Conv1D, MaxPooling2D, AveragePooling1D\n",
    "from keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.regularizers import l2\n",
    "from keras.constraints import max_norm\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import layers, models, Model, optimizers\n",
    "import seaborn as sns\n",
    "from keras.utils import to_categorical\n",
    "from glob import glob\n",
    "import cv2\n",
    "from sklearn.utils import shuffle\n",
    "from keras.applications import VGG16\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from keras.utils import  to_categorical\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import multilabel_confusion_matrix, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = \"/Users/abiramvyas/Documents/My_Projects/Ravdess/audio_speech_actors_01-24\"\n",
    "dicts={'01' : 'neutral', '02' : 'calm', '03' : 'happy', '04' : 'sad', '05' : 'angry', '06' : 'fearful', '07' : 'disgust', '08' : 'surprised'}\n",
    "path_main=audio\n",
    "folders_main=os.listdir(path_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "class audio_processing:\n",
    "    \"\"\"\n",
    "    This class is mainly created to preprocess the entire audio data. This includes trimming the data, manipulating it \n",
    "    based on the user requirement. \n",
    "    ----------\n",
    "    Attributes\n",
    "    ----------\n",
    "    path_to_folder : str\n",
    "        path to access audio files\n",
    "    manipulate : str\n",
    "        audio manipulation - stretch, normal, noise\n",
    "    path_to_save : str\n",
    "        final path to save the file\n",
    "\n",
    "    ----------\n",
    "    Methods\n",
    "    ----------\n",
    "    preprocess_data(path_to_folder,manipulate,path_to_save)\n",
    "        accesses the files present inside a given path and readies the data to be given to mel_spectrogram\n",
    "    spectrogram_mod(load_path,manipulate,op_path,emotion)\n",
    "        modifies the audio data, adds additional features such as noise or stretch and saves the data as an img\n",
    "    noise(y)\n",
    "        induces white noise onto the audio\n",
    "    stretch(y)\n",
    "        increase the duration of the audio by stretching the amplitude\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,path_to_folder,manipulate,path_to_save) -> None: \n",
    "        \"\"\"\n",
    "        Constructs all the necessary attributes for the object\n",
    "        ----------\n",
    "        Parameters\n",
    "        ----------\n",
    "        path_to_folder : str\n",
    "            path to access audio files\n",
    "        manipulate : str\n",
    "            audio manipulation - stretch, normal, noise\n",
    "        path_to_save : str\n",
    "            final path to save the file\n",
    "        \"\"\"\n",
    "        self.path_to_folder = path_to_folder\n",
    "        self.manipulate = manipulate\n",
    "        self.path_to_save = path_to_save\n",
    "\n",
    "    def preprocess_data(self,path_to_folder,manipulate,path_to_save):\n",
    "        \"\"\"\n",
    "        Preprocessing of the audio file by navigating through the main folders and passing it to mel_spectrogram\n",
    "        ----------\n",
    "        Parameters\n",
    "        ----------\n",
    "        path_to_folder : str\n",
    "            path to access audio files\n",
    "        manipulate : str\n",
    "            audio manipulation - stretch, normal, noise\n",
    "        path_to_save : str\n",
    "            final path to save the file\n",
    "        -------\n",
    "        Returns\n",
    "        -------\n",
    "        Prints counter value as and when folders have been processed\n",
    "        \"\"\"\n",
    "        cnt = 0\n",
    "        for i in folders_main:\n",
    "            ip_path = path_to_folder+'/{0}'.format(folders)\n",
    "            if ip_path != path_to_folder + '/.DS_Store':\n",
    "                files_inside_folder = os.listdir(ip_path)\n",
    "                for j in files_inside_folder:\n",
    "                    num_vals = re.findall('\\d+',j)\n",
    "                    emotion = dicts[num_vals[2]]\n",
    "                    \n",
    "                    op_path = path_to_save + emotion + '/' + j + '.jpeg'\n",
    "                    load_path = '{0}/{1}'.format(ip_path,j)\n",
    "\n",
    "                    self.spectrogram_mod(load_path,manipulate,op_path,emotion)\n",
    "            cnt+=1\n",
    "            print(\"Folders processed is {0}/24\".format(cnt))\n",
    "\n",
    "    def spectrogram_mod(self,load_path,manipulate,path_to_save,emotion):\n",
    "        \"\"\"\n",
    "        Converts the audio to the Mel scale by passing it through a fast fourier transform. Finally saving the audio\n",
    "        as an image\n",
    "        ----------\n",
    "        Parameters\n",
    "        ----------\n",
    "        load_path : str\n",
    "            path to access audio files\n",
    "        manipulate : str\n",
    "            audio manipulation - stretch, normal, noise\n",
    "        path_to_save : str\n",
    "            final path to save the file\n",
    "        emotion : str\n",
    "            emotion label obtained from the files\n",
    "        -------\n",
    "        Returns\n",
    "        -------\n",
    "        Image of the audio files processed in a .jpeg format\n",
    "        \"\"\"\n",
    "        y, sample_rate = librosa.load(load_path)\n",
    "        ytrim,_ = librosa.effects.trim(y)\n",
    "        y = ytrim\n",
    "\n",
    "        if manipulate == \"normal\":\n",
    "            y = librosa.feature.melspectrogram(y=y, sr=sample_rate, n_mels=128,fmax=8000)\n",
    "            \n",
    "        elif manipulate == \"noise\":\n",
    "            y = self.noise(y)\n",
    "            y = librosa.feature.melspectrogram(y=y, sr=sample_rate, n_mels=128,fmax=8000)\n",
    "        else:\n",
    "            y = self.stretch(y,0.8)\n",
    "            y = librosa.feature.melspectrogram(y=y, sr=sample_rate, n_mels=128,fmax=8000)\n",
    "\n",
    "        db_spec = librosa.power_to_db(y)\n",
    "        librosa.display.specshow(db_spec, y_axis='mel', fmax=20000, x_axis='time')\n",
    "        \n",
    "        if os.path.isdir('/Users/abiramvyas/Documents/My_Projects/Ravdess/{0}/{1}/'.format(manipulate,emotion)):\n",
    "            \n",
    "            plt.savefig(path_to_save)\n",
    "        else:\n",
    "            os.makedirs('/Users/abiramvyas/Documents/My_Projects/Ravdess/{0}/{1}/'.format(manipulate,emotion))\n",
    "            plt.savefig(path_to_save)\n",
    "\n",
    "    def noise(self,y):\n",
    "        \"\"\"\n",
    "        Induces noise into the data\n",
    "        ----------\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : array\n",
    "            array of the audio data from files\n",
    "        -------\n",
    "        Returns\n",
    "        -------\n",
    "        Data with noise induced\n",
    "        \"\"\"\n",
    "        amp_noise = 0.01*np.random.uniform()*np.amax(y)\n",
    "        y = y + amp_noise *np.random.normal(size=y.shape[0])\n",
    "        return y\n",
    "\n",
    "    def stretch(self,y, rate):\n",
    "        \"\"\"\n",
    "        Induces stretch into the data\n",
    "        ----------\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : array\n",
    "            array of the audio data from files\n",
    "        -------\n",
    "        Returns\n",
    "        -------\n",
    "        Data stretched with rate coming from the user. Default is 0.8\n",
    "        \"\"\"\n",
    "        data = librosa.effects.time_stretch(y, rate = rate)\n",
    "        return data\n",
    "\n",
    "audio_process = audio_processing('/Users/abiramvyas/Documents/My_Projects/Ravdess/audio_speech_actors_01-24',\n",
    "\"normal\",\"/Users/abiramvyas/Documents/My_Projects/Ravdess/normal/\")\n",
    "audio_process.preprocess_data('/Users/abiramvyas/Documents/My_Projects/Ravdess/audio_speech_actors_01-24',\n",
    "\"normal\",\"/Users/abiramvyas/Documents/My_Projects/Ravdess/normal/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pre_modelling :\n",
    "    \"\"\"\n",
    "    This class is mainly created to get the data ready for modelling. This class includes functions to fetch the \n",
    "    manipulated data, and performing the train test split for modelling \n",
    "    ----------\n",
    "    Attributes\n",
    "    ----------\n",
    "    No specific input attributes needed\n",
    "    ----------\n",
    "    Methods\n",
    "    ----------\n",
    "    img_fetch(directory)\n",
    "        accesses the files present inside a given path and readies the data to be given to be split into train-test\n",
    "    train_test_data_split(img_,labels_)\n",
    "        Peforms a 70-30 split of the data based on the images obtained through the img_fetch function\n",
    "    run_steps(directory)\n",
    "        runs the img_fetch and the train test split functions and returns the X_train, X_test, Y_train, Y_test\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Constructs all the necessary attributes for the object\n",
    "        ----------\n",
    "        Parameters\n",
    "        ----------\n",
    "        No input parameters are needed\n",
    "        \"\"\"\n",
    "        self.Images = []\n",
    "        self.Labels = []\n",
    "        \n",
    "    def img_fetch(self,directory):\n",
    "        \"\"\"\n",
    "        Obtains the processed images from the desired location. It could be a local drive or google drive\n",
    "        ----------\n",
    "        Parameters\n",
    "        ----------\n",
    "        directory : str\n",
    "            Directory where all the preprocessed files are stored\n",
    "        -------\n",
    "        Returns\n",
    "        -------\n",
    "        Resized shuffled images to make them consistent and its labels to be used for train and test\n",
    "        \"\"\"\n",
    "        Images = []\n",
    "        Labels = []\n",
    "        label_ = ''\n",
    "\n",
    "        for labels in os.listdir(directory):\n",
    "            if labels != '.DS_Store':\n",
    "                if labels == 'angry':\n",
    "                    label_ = 'angry'\n",
    "                elif labels == 'calm':\n",
    "                    label_ = 'calm'\n",
    "                elif labels == 'disgust':\n",
    "                    label_ = 'disgust'\n",
    "                elif labels == 'fearful':\n",
    "                    label_ = 'fearful'\n",
    "                elif labels == 'happy':\n",
    "                    label_ = 'happy'\n",
    "                elif labels == 'neutral':\n",
    "                    label_ = 'neutral'\n",
    "                elif labels == 'sad':\n",
    "                    label_ = 'sad'\n",
    "                elif labels == 'surprised':\n",
    "                    label_ = 'surprised'\n",
    "\n",
    "                for i in os.listdir(directory+labels): \n",
    "                    img_ = cv2.imread(directory+labels+r'/'+i) \n",
    "                    img_ = cv2.resize(img_,(224,224))\n",
    "                    Images.append(img_)\n",
    "                    Labels.append(label_)\n",
    "\n",
    "        return shuffle(Images,Labels,random_state=200) \n",
    "\n",
    "    def run_steps(self):\n",
    "        \"\"\"\n",
    "        Runs all the steps needed prior to modelling. This will retrieve the data, calls the train test split function\n",
    "        and sends the input data for modelling\n",
    "        ----------\n",
    "        Parameters\n",
    "        ----------\n",
    "        directory : str\n",
    "            Directory where all the preprocessed files are stored\n",
    "        -------\n",
    "        Returns\n",
    "        -------\n",
    "        X_train, X_test, y_train, y_test\n",
    "        \"\"\"\n",
    "        \n",
    "        Images_norm, Labels_norm  = self.img_fetch('/Users/abiramvyas/Documents/My_Projects/Ravdess/normal/')\n",
    "        Images_stretch, Labels_stretch  = self.img_fetch('/Users/abiramvyas/Documents/My_Projects/Ravdess/stretch/') \n",
    "        Images_noise, Labels_noise = self.img_fetch('/Users/abiramvyas/Documents/My_Projects/Ravdess/noise/')\n",
    "\n",
    "        img_ = Images_norm + Images_noise + Images_stretch\n",
    "        labels_ = Labels_norm + Labels_noise + Labels_stretch\n",
    "        X_train, X_test, y_train, y_test, lb = self.train_test_data_split(img_,labels_)\n",
    "\n",
    "        return X_train, X_test, y_train, y_test \n",
    "\n",
    "    def train_test_data_split(self,img_,labels_):\n",
    "        \"\"\"\n",
    "        Performs the train test split of the data, one hot encodes the Y variable and normalizes the X variables\n",
    "        ----------\n",
    "        Parameters\n",
    "        ----------\n",
    "        img_ : arr\n",
    "            Array of various images - stretched, normal, noise induced\n",
    "        labels_ : str\n",
    "            Emotion labels to be one hot encoded\n",
    "        -------\n",
    "        Returns\n",
    "        -------\n",
    "        X_train, X_test, y_train, y_test variables processed and one hot encoded\n",
    "        \"\"\"\n",
    "        X_train, X_test, y_train, y_test = train_test_split(img_, labels_, test_size=0.3,\n",
    "                                                            random_state=22, stratify=labels_)\n",
    "        lb = LabelEncoder()\n",
    "        X_train=np.array(X_train)\n",
    "        X_test=np.array(X_test)\n",
    "        \n",
    "        X_train = X_train.astype('float32')\n",
    "        X_test = X_test.astype('float32')\n",
    "        X_train /= 255\n",
    "        X_test /= 255\n",
    "\n",
    "        y_train = to_categorical(lb.fit_transform(y_train))\n",
    "        y_test = to_categorical(lb.fit_transform(y_test))\n",
    "\n",
    "        return X_train, X_test, y_train, y_test,lb\n",
    "\n",
    "run_premodel_steps = pre_modelling()\n",
    "X_train, X_test, y_train, y_test,lb_  = run_premodel_steps.run_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "class modelling:\n",
    "    \"\"\"\n",
    "    This class is mainly created to perform the modelling process. This class includes functions to fetch the perform\n",
    "    transfer learning using VGG16 and fitting the extracted features onto a random forest classifier for predictions\n",
    "    ----------\n",
    "    Attributes\n",
    "    ----------\n",
    "    X_train : array\n",
    "        Training data\n",
    "    X_test : array\n",
    "        Test data\n",
    "    y_train : array\n",
    "        Label training data\n",
    "    y_test : array\n",
    "        Label test data\n",
    "    ----------\n",
    "    Methods\n",
    "    ----------\n",
    "    run_vgg16(X_train,X_test,y_train,y_test)\n",
    "        calls the VGG16 model and builds a CNN to fit our data\n",
    "    ensemble_classifier(vgg_transfer_model,X_train,X_test,y_train,y_test)\n",
    "        uses the VGG16 model and extracts its features to train a RF classifier\n",
    "    \"\"\"\n",
    "    def __init__(self,X_train,X_test,y_train,y_test) -> None:\n",
    "        \"\"\"\n",
    "        Constructs all the necessary attributes for the object\n",
    "        ----------\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : array\n",
    "            Training data\n",
    "        X_test : array\n",
    "            Test data\n",
    "        y_train : array\n",
    "            Label training data\n",
    "        y_test : array\n",
    "            Label test data\n",
    "        \"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "\n",
    "    def run_vgg16(self,X_train,X_test,y_train,y_test,lb_,):\n",
    "        \"\"\"\n",
    "        Calls a VGG16 model (pre-trained). It is specified that we want to train the last three layers of the data.\n",
    "        The output layers are added to the model using softmax as the output activation layer. Model is saved as well\n",
    "        ----------\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : array\n",
    "            Training data\n",
    "        X_test : array\n",
    "            Test data\n",
    "        y_train : array\n",
    "            Label training data\n",
    "        y_test : array\n",
    "            Label test data\n",
    "        -------\n",
    "        Returns\n",
    "        -------\n",
    "        Confusion matrix along with the y_yte\n",
    "        \"\"\"\n",
    "        vgg16_model = VGG16(weights='imagenet',\n",
    "                include_top=False,\n",
    "                input_shape=(224, 224, 3))\n",
    "\n",
    "        x_pretrained = vgg16_model.output\n",
    "        x_pretrained = Flatten()(x_pretrained)\n",
    "        x_pretrained = Dense(256, activation='relu')(x_pretrained)\n",
    "        x_pretrained = Dropout(0.1)(x_pretrained)\n",
    "        x_pretrained = Dense(256, activation='relu')(x_pretrained)\n",
    "        x_pretrained = Dense(len(lb.classes_), activation='softmax')(x_pretrained)\n",
    "        vgg_transfer_model = Model(inputs=vgg16_model.input, outputs=x_pretrained)\n",
    "\n",
    "        # I want to train the last three layers alone. Others I want to use from existing trained model\n",
    "        for layer in vgg_transfer_model.layers[:19]:\n",
    "            layer.trainable = False\n",
    "        \n",
    "        for i, layer in enumerate(vgg_transfer_model.layers):\n",
    "            print(i, layer.name, layer.trainable)\n",
    "        \n",
    "        lr= 5e-5\n",
    "        opt = tf.keras.optimizers.legacy.RMSprop(learning_rate=lr)\n",
    "        vgg_transfer_model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "        history = vgg_transfer_model.fit(X_train, y_train, batch_size = 16, epochs=50, validation_data=(X_test,y_test))\n",
    "        vgg_transfer_model.save(\"vgg16_base_v2.h5\")\n",
    "        cm,y_test,pred_rf = self.ensemble_classifier(vgg_transfer_model,X_train,X_test,y_train,y_test)\n",
    "        return cm, y_test, pred_rf\n",
    "\n",
    "    def ensemble_classifier(self,vgg_transfer_model,X_train,X_test,y_train,y_test):\n",
    "        \"\"\"\n",
    "        The VGG16 model's training features are extracted and is fit on a random forest classifier. This is done since\n",
    "        there is a lack of data and resources to run a stand-alone VGG16 model. This is done to improve the accuracy \n",
    "        of the model\n",
    "        ----------\n",
    "        Parameters\n",
    "        ----------\n",
    "        vgg_transfer_model : model_obj\n",
    "            The VGG16 model that has been fit on the data\n",
    "        X_train : array\n",
    "            Training data\n",
    "        X_test : array\n",
    "            Test data\n",
    "        y_train : array\n",
    "            Label training data\n",
    "        y_test : array\n",
    "            Label test data\n",
    "        -------\n",
    "        Returns\n",
    "        -------\n",
    "        The confusion matrix post prediction of the test data, the y_test and the rf_predictions\n",
    "        \"\"\"\n",
    "        feature_extractor = vgg_transfer_model.predict(X_train)\n",
    "        features = feature_extractor.reshape(feature_extractor.shape[0], -1)\n",
    "        \n",
    "        RF_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        RF_model.fit(features, y_train)\n",
    "\n",
    "        X_test_feature = vgg_transfer_model.predict(X_test)\n",
    "        X_test_features = X_test_feature.reshape(X_test_feature.shape[0], -1)\n",
    "\n",
    "        prediction_RF = RF_model.predict(X_test_features)\n",
    "        cm = multilabel_confusion_matrix(y_test, prediction_RF)\n",
    "        pickle.dump(RF_model,open(\"rf_model.sav\",\"wb\"))\n",
    "        confusion_matrix_final =classification_report(\n",
    "        y_test,\n",
    "        prediction_RF,\n",
    "        )\n",
    "        return(confusion_matrix_final,y_test,prediction_RF)\n",
    "\n",
    "run_modelling = modelling(X_train,X_test,y_train,y_test)\n",
    "final_result = run_modelling.run_vgg16(X_train,X_test,y_train,y_test,lb_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unseen forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Recording my voice through python using the package PyAudio. It is needed that the recording is stored in a .wav format.\n",
    "If there is no requirement to record the voice through python, a file can be uploaded by the user as well\n",
    "'''\n",
    "import pyaudio\n",
    "import wave\n",
    "\n",
    "chunk = 1024 \n",
    "sample_format = pyaudio.paInt16 \n",
    "channels = 1\n",
    "fs = 44100  # Record at 44100 samples per second\n",
    "seconds = 3\n",
    "filename = \"/Users/abiramvyas/Documents/My_Projects/av_op1.wav\"\n",
    "\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "print('Recording')\n",
    "\n",
    "stream = p.open(format=sample_format,\n",
    "                channels=channels,\n",
    "                rate=fs,\n",
    "                frames_per_buffer=chunk,\n",
    "                input=True)\n",
    "\n",
    "frames = [] \n",
    "\n",
    "\n",
    "for i in range(0, int(fs / chunk * seconds)):\n",
    "    data = stream.read(chunk)\n",
    "    frames.append(data)\n",
    "\n",
    "# Stop and close the stream \n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "\n",
    "p.terminate()\n",
    "\n",
    "print('Finished recording')\n",
    "\n",
    "# Save the recorded data as a WAV file\n",
    "wf = wave.open(filename, 'wb')\n",
    "wf.setnchannels(channels)\n",
    "wf.setsampwidth(p.get_sample_size(sample_format))\n",
    "wf.setframerate(fs)\n",
    "wf.writeframes(b''.join(frames))\n",
    "wf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fearful'"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Pre-processing the audio and saving it as a .jpeg image\n",
    "'''\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "import re\n",
    "\n",
    "y_new, sr_new = librosa.load('/Users/abiramvyas/Documents/My_Projects/av_op1.wav')\n",
    "yt_new,__=librosa.effects.trim(y_new)\n",
    "y_new=yt_new\n",
    "y_new = librosa.feature.melspectrogram(y=y_new, sr=sr_new, n_mels=128,fmax=8000)\n",
    "db_spec_new = librosa.power_to_db(y_new)\n",
    "librosa.display.specshow(db_spec_new, y_axis='mel', fmax=20000, x_axis='time')\n",
    "if os.path.isdir('/Users/abiramvyas/Documents/My_Projects/Ravdess/unseen/'):\n",
    "    plt.savefig('/Users/abiramvyas/Documents/My_Projects/Ravdess/unseen/new_recording2.jpeg')\n",
    "else:\n",
    "    os.makedirs('/Users/abiramvyas/Documents/My_Projects/Ravdess/unseen/')\n",
    "    plt.savefig('/Users/abiramvyas/Documents/My_Projects/Ravdess/unseen/new_recording2.jpeg')\n",
    "\n",
    "\n",
    "loaded_img = load_img('/Users/abiramvyas/Documents/My_Projects/Ravdess/unseen/new_recording2.jpeg',target_size=(224,224))\n",
    "\n",
    "img_arr = img_to_array(loaded_img)\n",
    "img_arr = img_arr/255\n",
    "img_arr_final = img_arr.reshape(1, img_arr.shape[0],img_arr.shape[1],img_arr.shape[2])\n",
    "\n",
    "loaded_model = load_model('/Users/abiramvyas/Documents/My_Projects/Ravdess/Results/vgg16_base.h5')\n",
    "rf_loaded = pickle.load(open(\"/Users/abiramvyas/Documents/My_Projects/Ravdess/Results/rf_model.sav\", 'rb'))\n",
    "\n",
    "X_test_feature = loaded_model.predict(img_arr_final)\n",
    "X_test_features = X_test_feature.reshape(X_test_feature.shape[0], -1)\n",
    "\n",
    "prediction_RF = rf_loaded.predict(X_test_features)\n",
    "\n",
    "emotion_dict = {0:'angry', 1:'calm', 2:'disgust', 3:'fearful', 4:'happy', 5:'neutral',6:'sad',7:'surprised'}\n",
    "location_of_emotion = int(np.where(prediction_RF == 1)[1])\n",
    "emotion_dict.get(location_of_emotion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output is given as an input to the Spotify process. And songs are then recommended for the same"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
